---
title: "class07"
author: "Sharlene Yang"
format: pdf
---

# Clustering

First let's make up some data to cluster so we can get a feel for these methods and how to work with them.

We can use the `rnorm()` function to get random numbers from a normal distribution around a given `mean`.

```{r}
hist(rnorm(5000, mean = 3))
```

Let's get 30 points with a mean of 3.

```{r}
tmp <- c(rnorm(30, mean= 3), rnorm(30, mean = -3)) 
tmp

```

Put two of these together:
```{r}
x <- cbind(x=tmp, y= rev(tmp))
x
plot(x)
```


## K-means clustering.

Very popular clustering method that we cab use with the `kmeans()` function in base R.

```{r}
km <- kmeans(x, centers = 2)
km
```


```{r}
km$cluster
```

```{r}
km$center
```


```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch = 15)
```

>Q. Let's cluster into 3 groups or some `x` data and make a plot

```{r}
km <- kmeans(x, centers = 4)
plot(x, col=km$cluster)

```

# Hierarchial Clustering

We can use the `hclust()` for Hierarchical Clustering
Unlike `kmeans()`, where we could just pass in our data as input, we need to give give `hclust()` a "distance matrix".


We will use the `dist()` function to start with.

```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

```{r}
```


```{r}
plot(hc)
```


I can now "cut" my tree with the `cutree()` to yield a cluster membership vector.

```{r}
grps <- cutree(hc, h= 8)
grps
```

You can also tell `cutree()` to cut where it yields "k" groups.

```{r}
cutree(hc, k=2)
```

```{r}
plot(x, col=grps)
```


# Principal Component Analysis (PCA)

## Class 7 lab

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
nrow(x)
ncol(x)
```
There are 17 rows and 5 columns.

```{r}
View(x)
head(x)
tail(x)
```

To fix the name of the row, add  `row.name = 1` to the `read.csv`

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names= 1)
x
```

Checking the dimensions again
```{r}
dim(x)
```

>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I prefer the adding `row.names` because if you use the `-1` method and continuously run it, it will remove part of the table, which isn't what we want.

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

>Q3: Changing what optional argument in the above barplot() function results in the following plot? 

Changing the "besides" argument will change it to stacked bars.

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

>Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
```

For the code, it graphs the countries on the x-axis or the y-axis. It compares the amount of products consumed/ used in each country. For example, row 1 column 2, it shows England on the y-axis and Wales on the x-axis. If a given point lies on the diagonal for a given plot, it means that both countries have used about the same amount of the product.

>Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

Main difference is that is has the distinct blue dot near the 1000 mark.

The main PCA function in base R is called `prcomp()` it expects the transpose of our data.

```{r}
pca <- prcomp(t(x))
summary(pca)
```

```{r}
attributes(pca)
```

```{r}
pca$x
```

```{r}
plot(pca$x[,1], pca$x[,2], col=c("orange", "red", "blue", "darkgreen"), pch= 16)
```

